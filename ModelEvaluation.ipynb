{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model evaluation\n",
    "\n",
    "There are 3 different approaches to evaluate the quality of predictions of a model:\n",
    "* Estimator score method: Estimators have a score method providing a default evaluation criterion for the problem they are designed to solve. This is not discussed on this page, but in each estimator’s documentation.\n",
    "* Scoring parameter: Model-evaluation tools using cross-validation (such as model_selection.cross_val_score and model_selection.GridSearchCV) rely on an internal scoring strategy. This is discussed in the section The scoring parameter: defining model evaluation rules.\n",
    "* Metric functions: The metrics module implements functions assessing prediction error for specific purposes. These metrics are detailed in sections on Classification metrics, Multilabel ranking metrics, Regression metrics and Clustering metrics.\n",
    "\n",
    "Finally, Dummy estimators are useful to get a baseline value of those metrics for random predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The scoring parameter\n",
    "\n",
    "Model selection and evaluation using tools, such as model_selection.GridSearchCV and model_selection.cross_val_score, take a scoring parameter that controls what metric they apply to the estimators evaluated.\n",
    "\n",
    "For the most common use cases, you can designate a scorer object with the scoring parameter; the table below shows all possible values. All scorer objects follow the convention that higher return values are better than lower return values. Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, are available as neg_mean_squared_error which return the negated value of the metric.\n",
    "\n",
    "<table border=\"1\" class=\"docutils\">\n",
    "<colgroup>\n",
    "<col width=\"26%\">\n",
    "<col width=\"40%\">\n",
    "<col width=\"33%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">Scoring</th>\n",
    "<th class=\"head\">Function</th>\n",
    "<th class=\"head\">Comment</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td><strong>Classification</strong></td>\n",
    "<td>&nbsp;</td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>‘accuracy’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\" title=\"sklearn.metrics.accuracy_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.accuracy_score</span></code></a></td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>‘average_precision’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score\" title=\"sklearn.metrics.average_precision_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.average_precision_score</span></code></a></td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>‘f1’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\" title=\"sklearn.metrics.f1_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.f1_score</span></code></a></td>\n",
    "<td>for binary targets</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>‘f1_micro’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\" title=\"sklearn.metrics.f1_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.f1_score</span></code></a></td>\n",
    "<td>micro-averaged</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>‘f1_macro’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\" title=\"sklearn.metrics.f1_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.f1_score</span></code></a></td>\n",
    "<td>macro-averaged</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>‘f1_weighted’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\" title=\"sklearn.metrics.f1_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.f1_score</span></code></a></td>\n",
    "<td>weighted average</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>‘f1_samples’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\" title=\"sklearn.metrics.f1_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.f1_score</span></code></a></td>\n",
    "<td>by multilabel sample</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>‘neg_log_loss’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss\" title=\"sklearn.metrics.log_loss\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.log_loss</span></code></a></td>\n",
    "<td>requires <code class=\"docutils literal\"><span class=\"pre\">predict_proba</span></code> support</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>‘precision’ etc.</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score\" title=\"sklearn.metrics.precision_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.precision_score</span></code></a></td>\n",
    "<td>suffixes apply as with ‘f1’</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>‘recall’ etc.</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\" title=\"sklearn.metrics.recall_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.recall_score</span></code></a></td>\n",
    "<td>suffixes apply as with ‘f1’</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>‘roc_auc’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\" title=\"sklearn.metrics.roc_auc_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.roc_auc_score</span></code></a></td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><strong>Clustering</strong></td>\n",
    "<td>&nbsp;</td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>‘adjusted_rand_score’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score\" title=\"sklearn.metrics.adjusted_rand_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.adjusted_rand_score</span></code></a></td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><strong>Regression</strong></td>\n",
    "<td>&nbsp;</td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>‘neg_mean_absolute_error’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error\" title=\"sklearn.metrics.mean_absolute_error\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.mean_absolute_error</span></code></a></td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>‘neg_mean_squared_error’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error\" title=\"sklearn.metrics.mean_squared_error\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.mean_squared_error</span></code></a></td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>‘neg_median_absolute_error’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error\" title=\"sklearn.metrics.median_absolute_error\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.median_absolute_error</span></code></a></td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>‘r2’</td>\n",
    "<td><a class=\"reference internal\" href=\"generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score\" title=\"sklearn.metrics.r2_score\"><code class=\"xref py py-func docutils literal\"><span class=\"pre\">metrics.r2_score</span></code></a></td>\n",
    "<td>&nbsp;</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07490352 -0.16449405 -0.06685511]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ef0095f6e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wrong_choice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/nate/Desktop/scikit-learn-tutorial/env/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mcv_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;31m# We clone the estimator to make sure that all the folds are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nate/Desktop/scikit-learn-tutorial/env/lib/python2.7/site-packages/sklearn/metrics/scorer.pyc\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    250\u001b[0m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhas_scoring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m# Heuristic to ensure user has not passed a metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nate/Desktop/scikit-learn-tutorial/env/lib/python2.7/site-packages/sklearn/metrics/scorer.pyc\u001b[0m in \u001b[0;36mget_scorer\u001b[0;34m(scoring)\u001b[0m\n\u001b[1;32m    209\u001b[0m             raise ValueError('%r is not a valid scoring value. '\n\u001b[1;32m    210\u001b[0m                              \u001b[0;34m'Valid options are %s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                              % (scoring, sorted(scorers)))\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf = svm.SVC(probability=True, random_state=0)\n",
    "print cross_val_score(clf, X, y, scoring='neg_log_loss') \n",
    "\n",
    "model = svm.SVC()\n",
    "cross_val_score(model, X, y, scoring='wrong_choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "####  Defining your scoring strategy from metric functions\n",
    "\n",
    "The module sklearn.metric also exposes a set of simple functions measuring a prediction error given ground truth and prediction:\n",
    "\n",
    "* functions ending with _score return a value to maximize, the higher the better.\n",
    "* functions ending with _error or _loss return a value to minimize, the lower the better. When converting into a scorer object using make_scorer, set the greater_is_better parameter to False (True by default; see the parameter description below).\n",
    "\n",
    "Metrics available for various machine learning tasks are detailed in sections below.\n",
    "\n",
    "Many metrics are not given names to be used as scoring values, sometimes because they require additional parameters, such as fbeta_score. In such cases, you need to generate an appropriate scoring object. The simplest way to generate a callable object for scoring is by using make_scorer. That function converts metrics into callables that can be used for model evaluation.\n",
    "\n",
    "One typical use case is to wrap an existing metric function from the library with non-default values for its parameters, such as the beta parameter for the fbeta_score function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The second use case is to build a completely custom scorer object from a simple python function using make_scorer, which can take several parameters:\n",
    "\n",
    "* the python function you want to use (my_custom_loss_func in the example below)\n",
    "* whether the python function returns a score (greater_is_better=True, the default) or a loss (greater_is_better=False). If a loss, the output of the python function is negated by the scorer object, conforming to the cross validation convention that scorers return higher values for better models.\n",
    "* for classification metrics only: whether the python function you provided requires continuous decision certainties (needs_threshold=True). The default value is False.\n",
    "* any additional parameters, such as beta or labels in f1_score.\n",
    "\n",
    "Here is an example of building custom scorers, and of using the greater_is_better parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69314718055994529"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def my_custom_loss_func(ground_truth, predictions):\n",
    "    diff = np.abs(ground_truth - predictions).max()\n",
    "    return np.log(1 + diff)\n",
    "\n",
    "# loss_func will negate the return value of my_custom_loss_func,\n",
    "#  which will be np.log(2), 0.693, given the values for ground_truth\n",
    "#  and predictions defined below.\n",
    "loss  = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "ground_truth = [[1, 1]]\n",
    "predictions  = [0, 1]\n",
    "from sklearn.dummy import DummyClassifier\n",
    "# What is the dummy classifier!?! Wait one second\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf = clf.fit(ground_truth, predictions)\n",
    "loss(clf,ground_truth, predictions) \n",
    "\n",
    "score(clf,ground_truth, predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Other scoring functions\n",
    "\n",
    "There are so many different scoring functions that there is no way that we are going to go over all of them. But we will go over some extremely useful ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "The confusion_matrix function evaluates classification accuracy by computing the confusion matrix.\n",
    "\n",
    "By definition, entry i, j in a confusion matrix is the number of observations actually in group i, but predicted to be in group j. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "tn, fp, fn, tp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Classification Report\n",
    "\n",
    "The classification_report function builds a text report showing the main classification metrics. Here is a small example with custom target_names and inferred labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.67      1.00      0.80         2\n",
      "    class 1       0.00      0.00      0.00         1\n",
      "    class 2       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.67      0.60      0.59         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 0]\n",
    "y_pred = [0, 0, 2, 1, 0]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Dummy Estimators\n",
    "\n",
    "This was an interesting choice to put this here, but it did not feel right in any of the other sections.\n",
    "\n",
    "When doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of thumb. DummyClassifier implements several such simple strategies for classification:\n",
    "* stratified generates random predictions by respecting the training set class distribution.\n",
    "* most_frequent always predicts the most frequent label in the training set.\n",
    "* prior always predicts the class that maximizes the class prior (like most_frequent) and predict_proba returns the class prior.\n",
    "* uniform generates predictions uniformly at random.\n",
    "* constant always predicts a constant label that is provided by the user. A major motivation of this method is F1-scoring, when the positive class is in the minority.\n",
    "\n",
    "Note that with all these strategies, the predict method completely ignores the input data!\n",
    "To illustrate DummyClassifier, first let’s create an imbalanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "y[y != 1] = -1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's compare the SVC to the most_frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63157894736842102"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57894736842105265"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent',random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Right, we don't do much better! But with a simple kernel change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97368421052631582"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "DummyRegressor also implements four simple rules of thumb for regression:\n",
    "* mean always predicts the mean of the training targets.\n",
    "* median always predicts the median of the training targets.\n",
    "* quantile always predicts a user provided quantile of the training targets.\n",
    "* constant always predicts a constant value that is provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
